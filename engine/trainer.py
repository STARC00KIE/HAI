import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import os
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import log_loss
from models.base_model import BaseModel # ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤
from datetime import datetime

"""
íŠ¹ì§•:

-BaseModel ì§ì ‘ í˜¸ì¶œ    : ëª¨ë¸ ì´ë¦„ê³¼ í´ë˜ìŠ¤ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìœ ì—°í•œ ìƒì„±
- ì–¼ë¦¬ìŠ¤í† í•‘ ë‚´ì¥   :log_loss ê¸°ì¤€, PATIENCE íšŸìˆ˜ ì´ìƒ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
- í‰ê°€ ì§€í‘œ ë‹¤ì–‘    : val loss, accuracy, log_loss ëª¨ë‘ ì¶œë ¥
- best model ì €ì¥   : ./pth/ ë””ë ‰í† ë¦¬ì— ìë™ ì €ì¥
- epochë‹¹ train log ì €ì¥: ./log/ ë””ë ‰í† ë¦¬ì— ê°œë³„ ëª¨ë¸, ì „ì²´ ëª¨ë¸ ì €ì¥   
"""

class Trainer:
    def __init__(self, model_name, class_names, train_loader, val_loader, device, cfg):
        """
        Trainer ê°ì²´ ì´ˆê¸°í™”

        Args:
            model_name (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (timm ê¸°ë°˜)
            class_names (List[str]): ë¶„ë¥˜í•  í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            train_loader (DataLoader): í•™ìŠµìš© ë°ì´í„° ë¡œë”
            val_loader (DataLoader): ê²€ì¦ìš© ë°ì´í„° ë¡œë”
            device (torch.device): í•™ìŠµì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
            cfg (dict): í•™ìŠµ ì„¤ì •ì´ ë‹´ê¸´ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.model_name = model_name
        self.class_names = class_names
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.cfg = cfg
        self.num_classes = len(class_names)

        # ëª¨ë¸ ì •ì˜ ë° ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.model = BaseModel(model_name=self.model_name, num_classes=self.num_classes).to(self.device)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=cfg['LEARNING_RATE'])

        # Early stoppingì„ ìœ„í•œ ìµœì  logloss ê¸°ë¡
        self.best_logloss = float('inf')
        self.history = []  # ë¡œê·¸ ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸

        # ì²´í¬í¬ì¸íŠ¸, ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('./pth', exist_ok=True)
        os.makedirs('./logs', exist_ok=True)

    def train(self):
        """
        ì „ì²´ í•™ìŠµ ë£¨í”„ ìˆ˜í–‰ í•¨ìˆ˜
        - ê° epochë§ˆë‹¤ í•™ìŠµ â†’ ê²€ì¦
        - logloss í–¥ìƒ ì‹œ ëª¨ë¸ ì €ì¥
        - ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡ ë° ì €ì¥
        - early stopping ì ìš©
        """
        patience = self.cfg.get('PATIENCE', 5)
        counter = 0

        for epoch in range(self.cfg['EPOCHS']):
            self.model.train()
            total_train_loss = 0

            for images, labels in tqdm(self.train_loader, desc=f"[{self.model_name}] Epoch {epoch+1}/{self.cfg['EPOCHS']} - Training"):
                images, labels = images.to(self.device), labels.to(self.device)
                self.optimizer.zero_grad()
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
                total_train_loss += loss.item()

            avg_train_loss = total_train_loss / len(self.train_loader)

            # ê²€ì¦ ìˆ˜í–‰ ë° ì„±ëŠ¥ í‰ê°€
            avg_val_loss, val_accuracy, val_logloss = self.validate(epoch)

            # ì„±ëŠ¥ ê¸°ë¡
            self.history.append({
                'timestamp': datetime.now().strftime('%Y%m%d_%H%M'),
                'model': self.model_name,
                'img_size': self.cfg['IMG_SIZE'],
                'batch_size': self.cfg['BATCH_SIZE'],
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'val_acc': val_accuracy,
                'val_logloss': val_logloss
            })

            # ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ì‹œ ì €ì¥
            if val_logloss < self.best_logloss:
                self.best_logloss = val_logloss
                save_path = f"./pth/{self.cfg['CKPT_NAME']}"
                torch.save(self.model.state_dict(), save_path)
                print(f"âœ… Best model saved: {save_path} (logloss: {val_logloss:.4f})")
                counter = 0
            else:
                counter += 1
                print(f"âš ï¸ No improvement for {counter} epoch(s)")
                if counter >= patience:
                    print(f"â¹ Early stopping {self.model_name} at epoch {epoch+1}")
                    break

        # ë¡œê·¸ ì €ì¥ (ê°œë³„ ë¡œê·¸)
        log_df = pd.DataFrame(self.history)
        log_path = f"./logs/log_{self.cfg['FILE_NAME']}.csv"
        log_df.to_csv(log_path, index=False)
        print(f"ğŸ“„ ê°œë³„ í•™ìŠµ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_path}")

        # ë¡œê·¸ ì €ì¥ (í†µí•© ë¡œê·¸)
        all_log_path = './logs/all_logs.csv'
        log_df.to_csv(all_log_path, mode='a', header=not os.path.exists(all_log_path), index=False)
        print(f"ğŸ“„ ì „ì²´ í†µí•© ë¡œê·¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {all_log_path}")

    def validate(self, epoch):
        """
        ê²€ì¦ ë£¨í”„ ìˆ˜í–‰ í•¨ìˆ˜

        Args:
            epoch (int): í˜„ì¬ epoch ë²ˆí˜¸

        Returns:
            avg_val_loss (float): í‰ê·  ê²€ì¦ ì†ì‹¤
            val_accuracy (float): ê²€ì¦ ì •í™•ë„ (%)
            val_logloss (float): log-loss ì ìˆ˜
        """
        self.model.eval()
        val_loss = 0
        correct, total = 0, 0
        all_probs, all_labels = [], []

        with torch.no_grad():
            for images, labels in tqdm(self.val_loader, desc=f"[{self.model_name}] Epoch {epoch+1}/{self.cfg['EPOCHS']} - Validation"):
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                val_loss += loss.item()

                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

                probs = F.softmax(outputs, dim=1)
                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        avg_val_loss = val_loss / len(self.val_loader)
        val_accuracy = 100 * correct / total
        val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(self.class_names))))
        print(f"âœ… [Val] Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.2f}%, LogLoss: {val_logloss:.4f}")
        return avg_val_loss, val_accuracy, val_logloss

